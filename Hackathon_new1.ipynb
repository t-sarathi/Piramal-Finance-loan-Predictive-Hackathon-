{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6013253d-7725-49f3-a87a-36c841186649",
   "metadata": {},
   "source": [
    "# Import the necessary libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd5a290d-2502-46e9-8586-38f335eef36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815c0816-76e4-46d5-9a43-4b65a4390a8a",
   "metadata": {},
   "source": [
    "# Load the training and test data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56592374-8639-47b3-9fb3-e882b37293b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tirtha Ghosh\\AppData\\Local\\Temp\\ipykernel_21696\\3609402953.py:2: DtypeWarning: Columns (126,128,143) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train1 = pd.read_csv('train_1.csv')\n",
      "C:\\Users\\Tirtha Ghosh\\AppData\\Local\\Temp\\ipykernel_21696\\3609402953.py:3: DtypeWarning: Columns (675,676,677) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train2 = pd.read_csv('train_2_1.csv')\n",
      "C:\\Users\\Tirtha Ghosh\\AppData\\Local\\Temp\\ipykernel_21696\\3609402953.py:4: DtypeWarning: Columns (675,676,677) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train3 = pd.read_csv('train_2_2.csv')\n"
     ]
    }
   ],
   "source": [
    "# Loading training datasets\n",
    "train1 = pd.read_csv('train_1.csv')\n",
    "train2 = pd.read_csv('train_2_1.csv')\n",
    "train3 = pd.read_csv('train_2_2.csv')\n",
    "\n",
    "# Loading test datasets\n",
    "test1 = pd.read_csv('test_1.csv')\n",
    "test2 = pd.read_csv('test_2_1.csv')\n",
    "test3 = pd.read_csv('test_2_2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9577cda1-6096-4e65-86eb-8921101e3754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train1.shape-------> (100000, 168)\n",
      "Train2_1.shape-----> (90300, 678)\n",
      "Train2_2.shape-----> (47976, 678)\n",
      "Test1.shape-------> (100000, 167)\n",
      "Test2_1.shape-----> (81300, 678)\n",
      "Test2_2.shape-----> (48085, 678)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the training and test datasets\n",
    "print('Train1.shape------->', train1.shape)\n",
    "print('Train2_1.shape----->', train2.shape)\n",
    "print('Train2_2.shape----->', train3.shape)\n",
    "\n",
    "print('Test1.shape------->', test1.shape)\n",
    "print('Test2_1.shape----->', test2.shape)\n",
    "print('Test2_2.shape----->', test3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0599f672-42ec-4e05-8973-e4baadb22c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the categorical columns from the train1 dataset\n",
    "categorical_columns = train1.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Select the categorical columns from the test1 dataset\n",
    "test_categorical_columns = test1.select_dtypes(include=['object', 'category']).columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1d79904-c8d6-4c00-9415-9419b007edcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the categorical columns from the train1 dataset\n",
    "df = train1.drop(columns=categorical_columns)\n",
    "\n",
    "# Drop the categorical columns from the test1 dataset\n",
    "test_df = test1.drop(columns=test_categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1808df1-bf8f-43be-8516-2a5921f7a512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label          0\n",
      "col_1          0\n",
      "col_2          0\n",
      "col_3          0\n",
      "col_4          0\n",
      "           ...  \n",
      "col_159     1087\n",
      "col_160     1087\n",
      "col_162        0\n",
      "col_163    50277\n",
      "col_164     1096\n",
      "Length: 152, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking the number of null values in each column of the train set\n",
    "null_values = df.isnull().sum()\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838f514-cc93-4ccf-b8af-a53646ce2cad",
   "metadata": {},
   "source": [
    "## Identify features that are highly correlated with the target column:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d8fae7e-e27a-4af0-92c9-bd12a5ccf50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Features Based on Correlation: Index(['col_128', 'col_24', 'col_25', 'col_104', 'col_68', 'col_54', 'col_122',\n",
      "       'col_69', 'col_50', 'col_48', 'col_26', 'col_130', 'col_52', 'col_2',\n",
      "       'col_147', 'col_115', 'col_55', 'col_53', 'col_49', 'col_46', 'col_27',\n",
      "       'col_3', 'col_16', 'col_132', 'col_45', 'col_127', 'col_47', 'col_66',\n",
      "       'col_60', 'col_129', 'col_56', 'col_57', 'col_63', 'col_67', 'col_28',\n",
      "       'col_77', 'col_29', 'col_76', 'col_155', 'col_131', 'col_71', 'col_74',\n",
      "       'col_75', 'col_51', 'col_133', 'col_73', 'col_23', 'col_134', 'col_11',\n",
      "       'col_4', 'col_18', 'col_136', 'col_105', 'col_135', 'col_58', 'col_138',\n",
      "       'col_106', 'col_72', 'col_109', 'col_137', 'col_159', 'col_139',\n",
      "       'col_107', 'col_142', 'col_110', 'col_78', 'col_19', 'col_70', 'col_89',\n",
      "       'col_91', 'col_93', 'col_22', 'col_20', 'col_163', 'col_124', 'col_65',\n",
      "       'col_126', 'col_87', 'col_112', 'col_120', 'col_101', 'col_119',\n",
      "       'col_88', 'col_96', 'col_38', 'col_154', 'col_83', 'col_85', 'col_99',\n",
      "       'col_80', 'col_40', 'col_98', 'col_44', 'col_42', 'col_41', 'col_90',\n",
      "       'col_43', 'col_14', 'col_92'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame and 'target' is the column you are trying to predict\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Get the correlations with the target variable\n",
    "correlation_with_target = correlation_matrix['label'].sort_values(ascending=False)\n",
    "\n",
    "# Select the top features based on their absolute correlation values\n",
    "f = correlation_with_target.index[1:100]  # Select top 10 features excluding the target itself\n",
    "\n",
    "print(\"Top Features Based on Correlation:\", f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5bb17e2-eda2-452c-96c2-db061444ee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=[ 'col_24', 'col_25', 'col_68', 'col_54', 'col_122',\n",
    "       'col_69', 'col_50', 'col_48', 'col_26',  'col_2',\n",
    "       'col_147', 'col_127', 'col_66',\n",
    "       'col_60', 'col_63', 'col_67',  'col_76',  'col_74',\n",
    "       'col_75',  'col_73',  'col_58',  'col_72', 'col_142',  'col_70', 'col_89',\n",
    "       'col_91', 'col_93', 'col_22', 'col_163', 'col_124', \n",
    "       'col_126', 'col_119',\n",
    "        'col_83', 'col_85', 'col_99',\n",
    "       'col_43', 'col_14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d061440-3c28-402a-94ed-2c6729d4240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset in to the X and Y column :\n",
    "X=df[f]\n",
    "Y=df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d84e18a-8ca0-45f3-b8f2-cdece7a6bdad",
   "metadata": {},
   "source": [
    "# Impute the missing values (NaNs) on train data :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27f16a85-f031-42d9-9f9c-cc3cfa746c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Initialize the KNN Imputer with k=5 neighbors\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Fit and transform the datab\n",
    "impute_df= pd.DataFrame(knn_imputer.fit_transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "786ecca1-22fe-4908-a388-28fdfd7f69a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 37)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df123=impute_df\n",
    "df123.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a2a2d4-80ef-41df-87dc-bc7d1a26d5d8",
   "metadata": {},
   "source": [
    "### Identify the range of features that provide the highest cross-validation score:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4ab175-d881-49e4-9ac2-60dd673d5e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(40,100):\n",
    "    l=f[:i]\n",
    "    \n",
    "    X=df[l]\n",
    "    Y=df['label']\n",
    "    \n",
    "    # Example: Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize RandomForestClassifier\n",
    "    rf_clf = RandomForestClassifier(n_estimators =824,min_samples_split=10,min_samples_leaf=1,max_features='sqrt',\n",
    "                                    max_depth=20,bootstrap=False)\n",
    "    #'n_estimators': 824, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': False\n",
    "    # Output cross-validation scores\n",
    "    cv_scores = cross_val_score(rf_clf, X_train, y_train, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    \n",
    "    print(i,'-------->',\"Mean CV AUC-ROC Score:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f17ae76-8943-4b3d-8628-a5737f60e830",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning of RandomForest model using the optuna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c678531-135d-44c8-8d6b-944ccdb0cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure 'X' and 'Y' are your features and target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameter search space\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    max_features = trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])\n",
    "    bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "\n",
    "    # Define the Random Forest model with suggested hyperparameters\n",
    "    rf_clf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,  # Updated to avoid 'auto'\n",
    "        bootstrap=bootstrap,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities on the test set and calculate AUC-ROC score\n",
    "    score = roc_auc_score(y_test, rf_clf.predict_proba(X_test)[:, 1])\n",
    "    return score\n",
    "\n",
    "# Create a study object and optimize it\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004de783-5f0a-4b7b-9480-cbe7a6b11bac",
   "metadata": {},
   "source": [
    "The Optuna code took around 2 hours to execute and identify the best features. Since I ran the code in a separate Python file, the execution result is not displayed here. However, the optimal hyperparameters for my model are as follows: \r",
    "('\n",
    "n_estimator's: 824,' min_samples_spli': 10,' min_samples_lea': 1,' max_feature's: 'sqrt',' max_dept'h: 20,' bootstra'p: False).\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d9b28-9027-4d9e-9c9d-032def0655ba",
   "metadata": {},
   "source": [
    "## Train the model after hyper parameter tuning :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0071a2e-f0e3-4507-90a5-17fcaf45885c",
   "metadata": {},
   "source": [
    "Out of the 100 best features I extracted, the first 83 features yielded the highest cross-validation score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d6e73e6-fd44-4c0a-8792-06b7ae73bccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------> Mean CV AUC-ROC Score: 0.8178833587994057\n"
     ]
    }
   ],
   "source": [
    "l=f[:83]\n",
    "    \n",
    "X1=df123[l]\n",
    "Y=df['label']\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "    # Example: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, Y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(n_estimators =824,min_samples_split=10,min_samples_leaf=1,max_features='sqrt',\n",
    "                                    max_depth=20,bootstrap=False)\n",
    "    #'n_estimators': 824, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': False\n",
    "    # Output cross-validation scores\n",
    "cv_scores = cross_val_score(rf_clf, X_train, y_train, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    \n",
    "print('-------->',\"Mean CV AUC-ROC Score:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb2d811-37e4-4423-9a33-593e176e5dbd",
   "metadata": {},
   "source": [
    "## Extract the features from the test data that were used to train the model:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a26757-c3cc-47c8-bec6-17f7d09149dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1=test1[l]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14304133-8efb-4b11-aec4-116b2a3082f5",
   "metadata": {},
   "source": [
    "# Impute the missing values (NaNs) on test data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9ca285-a145-4563-8eb1-d3b8a90ad808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Initialize the KNN Imputer with k=5 neighbors\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Fit and transform the datab\n",
    "test_X1 = pd.DataFrame(knn_imputer.fit_transform(test_df1), columns=test_df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4c178c-a2a9-4019-bc88-b349f0715582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the test data as a CSV file on my system for future reference:\n",
    "test_X1.to_csv('last_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a7a604ea-5b6b-4f62-af9a-74924fce3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('last_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a41edbb5-8bd4-4974-b1a6-05525d67ac90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 99)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eb0c0d-133c-43dd-9865-c12a00f5516b",
   "metadata": {},
   "source": [
    "## Execute the model on the test data and save the predicted values to a CSV file:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d575df8c-25c1-483a-89e2-6c1877eff34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "l=f[:83]\n",
    "    \n",
    "X=df[l]\n",
    "Y=df['label']\n",
    "test1=test1[X.columns]\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X is your feature set and Y is your target (label)\n",
    "# Split the dataset into training and testing sets (if not done already)\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest Classifier with valid parameters\n",
    "model= RandomForestClassifier(n_estimators =824,min_samples_split=10,min_samples_leaf=1,max_features='sqrt',\n",
    "                                    max_depth=20,bootstrap=False)\n",
    "\n",
    "# Train the model on the training data\n",
    "rf_clf.fit(X, Y)\n",
    "\n",
    "# Load your new test data into a DataFrame (e.g., test_new)\n",
    "# Ensure it has the same columns as X\n",
    "# For example, test_new = pd.read_csv('new_test_data.csv')\n",
    "\n",
    "# Make predictions on the new test data\n",
    "predictions = rf_clf.predict_proba(test1)[:, 1]  # Predict probabilities for the positive class\n",
    "\n",
    "# Convert predictions to a list if needed\n",
    "predictions = list(predictions)\n",
    "\n",
    "# If you want to save the predictions to a CSV file\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "sub = test[['loan_id']].copy()\n",
    "sub['prob'] = predictions\n",
    "\n",
    "# Save to CSV\n",
    "sub.to_csv('RFC81.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24fde778-b938-426b-96e2-2f9a2417413a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------> Mean CV AUC-ROC Score: 0.8145764376037954\n"
     ]
    }
   ],
   "source": [
    "l=f[:83]\n",
    "    \n",
    "X1=df123[l]\n",
    "Y=df['label']\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "    # Example: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, Y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(n_estimators =824,min_samples_split=10,min_samples_leaf=1,max_features='sqrt',\n",
    "                                    max_depth=20,bootstrap=False)\n",
    "    #'n_estimators': 824, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': False\n",
    "    # Output cross-validation scores\n",
    "cv_scores = cross_val_score(rf_clf, X_train, y_train, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "    \n",
    "print('-------->',\"Mean CV AUC-ROC Score:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a6ec2a-af04-4ca4-959c-355abeff9dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
